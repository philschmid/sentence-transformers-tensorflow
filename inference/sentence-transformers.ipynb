{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Sentence-Transformer with TensorFlow\n",
    "\n",
    "In this blog, you will learn how to use a [Sentence Transformers](https://www.sbert.net/) model with TensorFlow and Keras. The blog will show you how to create a custom Keras model to load [Sentence Transformers](https://www.sbert.net/) models and run inference on it to create document embeddings. \n",
    "\n",
    "[Sentence Transformers](https://www.sbert.net/) is the state-of-the-art library for sentence, text, and image embeddings to build semantic textual similarity, semantic search, or paraphrase mining applications using BERT and Transformers üîé¬†1Ô∏è‚É£¬†‚≠êÔ∏è\n",
    "\n",
    "![SBERT](../assets/sentence-transformers.png)\n",
    "\n",
    "Paper: [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)\n",
    "\n",
    "The library is built on top of PyTorch and Hugging Face Transforemrs so it is compatible with PyTorch models and not with TensorFlow by default.\n",
    "But since Hugging Face Transformers is compatible with PyTorch and TensorFlow it is possible to load the raw Sentence Transformer models in Tensorflow. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup Development Environment](#1-setup-development-environment)\n",
    "2. [Create a custom TensorFlow Model](#2-create-a-custom-tensorflow-model)\n",
    "3. [Run inference and validate results](#3-run-inference-and-validate-results)\n",
    "4. [Create e2e model with tokenizer included](#4-create-e2e-model-with-tokenizer-included)\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Transformers, along with tensorflow-text and some other libraries. We are also installing `sentence-transformers` for later use to validate our model and results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing tensorflow extra due to incompatibility with conda and tensorflow-text https://github.com/tensorflow/text/issues/644\n",
    "!pip install transformers[tf] -q --upgrade \n",
    "!pip install sentence-transformers -q # needed for validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers torch -q # needed for validating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a custom TensorFlow Model\n",
    "\n",
    "When using `sentence-transformers` natively you can run inference by loading your model in the `SentenceTransformer` class and then calling the `.encode()` method. However this only works for PyTorch and we want to use TensorFlow. When calling `.encode()` on your PyTorch model SentenceTransformers will first do a forward pass through the vanilla Hugging Face `AutoModel` class then apply pooling and or normalization.\n",
    "\n",
    "This means if we want to use TensorFlow we can create a similar `TFSentenceTransformer` class, which does the same thing as `SentenceTransformer` but uses TensorFlow and Keras instead of PyTorch.\n",
    "\n",
    "Our first step is to create a custom TensorFlow model initalizes our `TFAutoModel` from Transformers and includes helper methods for `mean_pooling` and normalization. \n",
    "\n",
    "_Note: We focus in this example only on Sentence Transformers, which are not including any additional layers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 12:19:30.759970: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "class TFSentenceTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_name_or_path, **kwargs):\n",
    "        super(TFSentenceTransformer, self).__init__()\n",
    "        # loads transformers model\n",
    "        self.model = TFAutoModel.from_pretrained(model_name_or_path, **kwargs)\n",
    "\n",
    "    def call(self, inputs, normalize=True):\n",
    "        # runs model on inputs\n",
    "        model_output = self.model(inputs)\n",
    "        # Perform pooling. In this case, mean pooling.\n",
    "        embeddings = self.mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "        # normalizes the embeddings if wanted\n",
    "        if normalize:\n",
    "          embeddings = self.normalize(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = tf.cast(\n",
    "            tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)),\n",
    "            tf.float32\n",
    "        )\n",
    "        return tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) / tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max)\n",
    "\n",
    "    def normalize(self, embeddings):\n",
    "      embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1)\n",
    "      return embeddings   \n",
    "\n",
    "# TODO check for batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our model by selecting and loading a Sentence Transformer from the [Hugging Face Hub](https://huggingface.co/models?library=sentence-transformers,tf&sort=downloads).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = TFSentenceTransformer(model_id)\n",
    "\n",
    "# Run inference & create embeddings\n",
    "payload = [\"This is a sentence embedding\",\n",
    "           \"This is another sentence embedding\"]\n",
    "encoded_input = tokenizer(payload, padding=True, truncation=True, return_tensors='tf')\n",
    "sentence_embedding = model(encoded_input)\n",
    "\n",
    "print(sentence_embedding.shape)\n",
    "# <tf.Tensor: shape=(2, 384), dtype=float32, numpy= array([[[ 3.37564945e-02,  4.20359336e-02,  6.31270036e-02,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run inference and validate results\n",
    "\n",
    "After we have now successfully created our custom TensorFlow model `TFSentenceTransformer` we should compare our results to the results from the original Sentence Transformers model.\n",
    "\n",
    "Therefore are we loading our model using `sentence-transformers` and comparing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are results close: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "compare_input = \"This is a sentence embedding, which we will compare between PyTorch and TensorFlow\"\n",
    "\n",
    "# loading sentence transformers\n",
    "st_model = SentenceTransformer(model_id,device=\"cpu\")\n",
    "\n",
    "# run inference with sentence transformers\n",
    "st_embeddings = st_model.encode(compare_input)\n",
    "# run inference with TFSentenceTransformer\n",
    "encoded_input = tokenizer(compare_input, return_tensors=\"tf\")\n",
    "tf_embeddings =  model(encoded_input)\n",
    "\n",
    "# compare embeddings\n",
    "are_results_close = np.allclose(tf_embeddings.numpy()[0],st_embeddings, atol=7e-8)\n",
    "print(f\"Are results close: {are_results_close}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created sentence embeddings from our `TFSentenceTransformer` model have less then `0.00000007` difference with the original Sentence Transformers model. This is good enough to validate our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create e2e model with tokenizer included\n",
    "\n",
    "One difference between the original Sentence Transformers model and the custom TensorFlow model is that the original model does include a tokenizer. This is not included in the custom TensorFlow model. \n",
    "The orginial sentence transformer model is using `AutoTokenizer` to tokenize the sentences as well meaning it is not included in our model graph. But we can create a custom model for `BERT` that includes [FastBertTokenizer](https://www.tensorflow.org/text/api_docs/python/text/FastBertTokenizer) from TensorFlow Text. This will allow us to use the tokenizer in our model graph. \n",
    "The [FastBertTokenizer](https://www.tensorflow.org/text/api_docs/python/text/FastBertTokenizer) got integrated into transformers as [TFBERTTokenizer](https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/bert#transformers.TFBertTokenizer) and works with the Hugging Face assets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, TFBertTokenizer\n",
    "\n",
    "\n",
    "class E2ESentenceTransformer(tf.keras.Model):\n",
    "    def __init__(self, model_name_or_path, **kwargs):\n",
    "        super().__init__()\n",
    "        # loads the in-graph tokenizer\n",
    "        self.tokenizer = TFBertTokenizer.from_pretrained(model_name_or_path, **kwargs)\n",
    "        # loads our TFSentenceTransformer   \n",
    "        self.model = TFSentenceTransformer(model_name_or_path, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # runs tokenization and creates embedding\n",
    "        tokenized = self.tokenizer(inputs)\n",
    "        return self.model(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our `E2ESentenceTransformer` model that includes the tokenizer and run inference on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (1, 384)\n",
      "Model: \"e2e_sentence_transformer_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_bert_tokenizer_30 (TFBer  multiple                 0         \n",
      " tTokenizer)                                                     \n",
      "                                                                 \n",
      " tf_sentence_transformer_59   multiple                 22713216  \n",
      " (TFSentenceTransformer)                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,713,216\n",
      "Trainable params: 22,713,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# hugging face model id\n",
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "# loading model with tokenizer and sentence transformer\n",
    "e2e_model = E2ESentenceTransformer(model_id)\n",
    "\n",
    "# run inference\n",
    "payload = \"This is a sentence embedding\"\n",
    "\n",
    "pred = e2e_model([payload])  # Pass strings straight to model!\n",
    "print(f\"output shape: {pred.shape}\")\n",
    "e2e_model.summary() # prints model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thats it. We have now successfully created a custom TensorFlow model that can load a Sentence Transformer model and run inference on it to create document embeddings. This will allow you to integrate Sentence Transformers into your existing and new TensorFlow projects and workflows. We validated that our model creates embeddings that are similar to the original Sentence Transformers model.\n",
    "\n",
    "And as bonus we looked into how to integrate the tokenization into our model graph as well and created an `E2ESentenceTransformer` model that includes the tokenizer for BERT models, which are achieving state-of-the-art performance on similar tasks.\n",
    "\n",
    "If you are interested into to deploy those models with TFServing let me know! \n",
    "\n",
    "---\n",
    "\n",
    "Thanks for reading. If you have any questions, feel free to contact me, through [Github](https://github.com/huggingface/transformers), or on the [forum](https://discuss.huggingface.co/c/sagemaker/17). You can also connect with me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ca8cb2299bb0a7ff024f6c656e300a1b291b973dc78c146fc518b49bc9e11a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
